W0212 10:05:18.425859 2126390 site-packages/torch/distributed/run.py:792] 
W0212 10:05:18.425859 2126390 site-packages/torch/distributed/run.py:792] *****************************************
W0212 10:05:18.425859 2126390 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0212 10:05:18.425859 2126390 site-packages/torch/distributed/run.py:792] *****************************************
----------------- Options ---------------
               batch_size: 2                             
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: False                         
                crop_size: 800                           	[default: 512]
                 dataroot: ../datasets/sperm_instance_800_opt	[default: None]
             dataset_mode: usseg                         	[default: template]
                direction: AtoB                          
             display_freq: 200                           
              distributed: False                         	[default: None]
                    epoch: latest                        
              epoch_count: 1                             
                 gan_mode: lsgan                         
                  gpu_ids: 4,5,6,0                       	[default: 0,1]
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 1                             
                  isTrain: True                          	[default: None]
                 lambda_A: 1.0                           
                 lambda_B: 1.0                           
                lambda_DA: 1.0                           
                lambda_DB: 1.0                           
                lambda_GA: 20.0                          	[default: 1.0]
                lambda_GB: 20.0                          	[default: 1.0]
                load_iter: 0                             	[default: 0]
                load_size: 800                           	[default: 512]
               local_rank: 0                             	[default: None]
                       lr: 0.0001                        	[default: 0.0002]
           lr_decay_iters: 150                           	[default: 50]
                lr_policy: step                          	[default: linear]
         max_dataset_size: inf                           
            max_instances: 10                            	[default: 5]
                    model: usseg                         	[default: cycle_gan]
                 n_epochs: 150                           	[default: 100]
           n_epochs_decay: 450                           	[default: 100]
               n_layers_D: 3                             
                     name: 800size_10ins                 	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: resnet_9blocks                
                      ngf: 64                            
               no_dropout: True                          
                  no_flip: True                          	[default: False]
                     norm: instance                      
              num_threads: 4                             	[default: 8]
                output_nc: 1                             
                    phase: train                         
                pool_size: 50                            
                   pregen: None                          
               preprocess: none                          	[default: resize_and_crop]
                   preseg: None                          
               print_freq: 100                           
                     rank: 0                             	[default: None]
             save_by_iter: False                         
          save_epoch_freq: 100                           
         save_latest_freq: 4000                          
           serial_batches: False                         
                   suffix:                               
                  use_amp: True                          	[default: False]
                  verbose: False                         
               world_size: 1                             	[default: None]
----------------- End -------------------
dataset [USSEGDataset] was created
Using DistributedSampler: 250 samples per GPU
DDP enabled: world_size=4, using DistributedSampler
The number of training images = 1000 (total), 250 per GPU
Some weights of SwinBackbone were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized: ['hidden_states_norms.stage1.bias', 'hidden_states_norms.stage1.weight', 'hidden_states_norms.stage2.bias', 'hidden_states_norms.stage2.weight', 'hidden_states_norms.stage3.bias', 'hidden_states_norms.stage3.weight', 'hidden_states_norms.stage4.bias', 'hidden_states_norms.stage4.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of SwinBackbone were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized: ['hidden_states_norms.stage1.bias', 'hidden_states_norms.stage1.weight', 'hidden_states_norms.stage2.bias', 'hidden_states_norms.stage2.weight', 'hidden_states_norms.stage3.bias', 'hidden_states_norms.stage3.weight', 'hidden_states_norms.stage4.bias', 'hidden_states_norms.stage4.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
pixel_encoder_params:  27520698
pixel_decoder_params:  5421504
transformer_params:  14412544
Mask2Former model initialized
pixel_encoder_params:  27520698
pixel_decoder_params:  5421504
transformer_params:  14412544
Mask2Former model initialized
Some weights of SwinBackbone were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized: ['hidden_states_norms.stage1.bias', 'hidden_states_norms.stage1.weight', 'hidden_states_norms.stage2.bias', 'hidden_states_norms.stage2.weight', 'hidden_states_norms.stage3.bias', 'hidden_states_norms.stage3.weight', 'hidden_states_norms.stage4.bias', 'hidden_states_norms.stage4.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
pixel_encoder_params:  27520698
pixel_decoder_params:  5421504
transformer_params:  14412544
Mask2Former model initialized
Some weights of SwinBackbone were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized: ['hidden_states_norms.stage1.bias', 'hidden_states_norms.stage1.weight', 'hidden_states_norms.stage2.bias', 'hidden_states_norms.stage2.weight', 'hidden_states_norms.stage3.bias', 'hidden_states_norms.stage3.weight', 'hidden_states_norms.stage4.bias', 'hidden_states_norms.stage4.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
pixel_encoder_params:  27520698
pixel_decoder_params:  5421504
transformer_params:  14412544
Mask2Former model initialized
initialize network with normal
initialize network with normal
initialize network with normal
initialize network with normal
model [USSEGModel] was created
---------- Networks initialized -------------
[Network G_A] Total number of parameters : 47.355 M
[Network G_B] Total number of parameters : 12.546 M
[Network D_A] Total number of parameters : 2.894 M
[Network D_B] Total number of parameters : 2.763 M
-----------------------------------------------
create image directory ./checkpoints/800size_10ins/images...
Start at:  1770890725.6360106
[rank1]:[W212 10:05:26.750056663 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W212 10:05:27.791103456 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W212 10:05:27.899387562 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W212 10:05:27.153734923 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
(epoch: 1, iters: 200, time: 0.424, data: 0.374) cycle_A: 55.573 D_A: 0.232 G_A: 6.747 D_B: 0.662 G_B: 13.654 cycle_B: 0.002 G: 75.977 D: 0.893 
(epoch: 1, iters: 400, time: 0.439, data: 0.000) cycle_A: 46.478 D_A: 0.263 G_A: 6.176 D_B: 0.467 G_B: 11.954 cycle_B: 0.000 G: 64.608 D: 0.731 
(epoch: 1, iters: 600, time: 0.426, data: 0.000) cycle_A: 46.488 D_A: 0.180 G_A: 7.505 D_B: 0.347 G_B: 12.300 cycle_B: 0.000 G: 66.293 D: 0.527 
(epoch: 1, iters: 800, time: 0.452, data: 0.000) cycle_A: 47.773 D_A: 0.168 G_A: 6.138 D_B: 0.295 G_B: 13.397 cycle_B: 0.000 G: 67.308 D: 0.463 
(epoch: 1, iters: 1000, time: 0.447, data: 0.000) cycle_A: 45.649 D_A: 0.206 G_A: 4.038 D_B: 0.357 G_B: 8.660 cycle_B: 0.001 G: 58.347 D: 0.562 
learning rate 0.0000500 -> 0.0000500
End of epoch 1 / 600 	 Time Taken: 115 sec
(epoch: 2, iters: 200, time: 0.441, data: 0.408) cycle_A: 46.882 D_A: 0.200 G_A: 5.050 D_B: 0.311 G_B: 8.540 cycle_B: 0.001 G: 60.473 D: 0.511 
(epoch: 2, iters: 400, time: 0.424, data: 0.000) cycle_A: 46.483 D_A: 0.204 G_A: 7.759 D_B: 0.255 G_B: 10.571 cycle_B: 0.000 G: 64.814 D: 0.460 
(epoch: 2, iters: 600, time: 0.427, data: 0.000) cycle_A: 43.405 D_A: 0.186 G_A: 8.224 D_B: 0.255 G_B: 10.089 cycle_B: 0.001 G: 61.719 D: 0.441 
(epoch: 2, iters: 800, time: 0.439, data: 0.000) cycle_A: 44.919 D_A: 0.189 G_A: 5.814 D_B: 0.246 G_B: 9.278 cycle_B: 0.000 G: 60.011 D: 0.435 
(epoch: 2, iters: 1000, time: 0.449, data: 0.000) cycle_A: 44.066 D_A: 0.234 G_A: 12.273 D_B: 0.243 G_B: 8.861 cycle_B: 0.000 G: 65.200 D: 0.477 
learning rate 0.0000500 -> 0.0000500
End of epoch 2 / 600 	 Time Taken: 111 sec
