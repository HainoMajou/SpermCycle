W0207 06:43:54.229113 2781032 site-packages/torch/distributed/run.py:792] 
W0207 06:43:54.229113 2781032 site-packages/torch/distributed/run.py:792] *****************************************
W0207 06:43:54.229113 2781032 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0207 06:43:54.229113 2781032 site-packages/torch/distributed/run.py:792] *****************************************
----------------- Options ---------------
               batch_size: 2                             
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: False                         
                crop_size: 800                           	[default: 512]
                 dataroot: ../datasets/sperm_instance_800_opt	[default: None]
             dataset_mode: usseg                         	[default: template]
                direction: AtoB                          
             display_freq: 200                           
              distributed: False                         	[default: None]
                    epoch: latest                        
              epoch_count: 1                             
         freeze_GA_epochs: 300                           
         freeze_GB_epochs: 300                           
                 gan_mode: lsgan                         
                  gpu_ids: 0,1,2,3,4,5,6,7               	[default: 0,1]
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 1                             
                  isTrain: True                          	[default: None]
                 lambda_A: 1.0                           
                 lambda_B: 1.0                           
                lambda_DA: 1.0                           
                lambda_DB: 1.0                           
                lambda_GA: 20.0                          	[default: 1.0]
                lambda_GB: 20.0                          	[default: 1.0]
                load_iter: 0                             	[default: 0]
                load_size: 800                           	[default: 512]
               local_rank: 0                             	[default: None]
                       lr: 4e-05                         	[default: 0.0002]
           lr_decay_iters: 150                           	[default: 50]
                lr_policy: step                          	[default: linear]
         max_dataset_size: inf                           
            max_instances: 10                            	[default: 5]
                    model: usseg                         	[default: cycle_gan]
                 n_epochs: 150                           	[default: 100]
           n_epochs_decay: 450                           	[default: 100]
               n_layers_D: 3                             
                     name: 800size_10ins_preseg_pregen   	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: resnet_9blocks                
                      ngf: 64                            
               no_dropout: True                          
                  no_flip: True                          	[default: False]
                     norm: instance                      
              num_threads: 4                             	[default: 8]
                output_nc: 1                             
                    phase: train                         
                pool_size: 50                            
                   pregen: checkpoints/800size_10ins_preseg/500_net_G_B.pth	[default: None]
               preprocess: none                          	[default: resize_and_crop]
                   preseg: checkpoints/800size_10ins_preseg/500_net_G_A.pth	[default: None]
               print_freq: 100                           
                     rank: 0                             	[default: None]
             save_by_iter: False                         
          save_epoch_freq: 100                           
         save_latest_freq: 4000                          
           serial_batches: False                         
                   suffix:                               
                  use_amp: True                          	[default: False]
                  verbose: False                         
               world_size: 1                             	[default: None]
----------------- End -------------------
dataset [USSEGDataset] was created
Using DistributedSampler: 125 samples per GPU
DDP enabled: world_size=8, using DistributedSampler
The number of training images = 1000 (total), 125 per GPU
Mask2Former model loaded from checkpoints/800size_10ins_preseg/500_net_G_A.pth
Mask2Former model loaded from checkpoints/800size_10ins_preseg/500_net_G_A.pth
Mask2Former model loaded from checkpoints/800size_10ins_preseg/500_net_G_A.pth
Mask2Former model loaded from checkpoints/800size_10ins_preseg/500_net_G_A.pth
Mask2Former model loaded from checkpoints/800size_10ins_preseg/500_net_G_A.pth
Mask2Former model loaded from checkpoints/800size_10ins_preseg/500_net_G_A.pth
Mask2Former model loaded from checkpoints/800size_10ins_preseg/500_net_G_A.pth
Mask2Former model loaded from checkpoints/800size_10ins_preseg/500_net_G_A.pth
initialize network with normal
Generator model loaded from checkpoints/800size_10ins_preseg/500_net_G_B.pth
Generator model loaded from checkpoints/800size_10ins_preseg/500_net_G_B.pth
Generator model loaded from checkpoints/800size_10ins_preseg/500_net_G_B.pth
Generator model loaded from checkpoints/800size_10ins_preseg/500_net_G_B.pth
Generator model loaded from checkpoints/800size_10ins_preseg/500_net_G_B.pth
Generator model loaded from checkpoints/800size_10ins_preseg/500_net_G_B.pth
Generator model loaded from checkpoints/800size_10ins_preseg/500_net_G_B.pth
Generator model loaded from checkpoints/800size_10ins_preseg/500_net_G_B.pth
initialize network with normal
initialize network with normal
initialize network with normal
model [USSEGModel] was created
---------- Networks initialized -------------
[Network G_A] Total number of parameters : 47.355 M
[Network G_B] Total number of parameters : 12.546 M
[Network D_A] Total number of parameters : 2.763 M
[Network D_B] Total number of parameters : 2.763 M
-----------------------------------------------
create image directory ./checkpoints/800size_10ins_preseg_pregen/images...
Start at:  1770446644.6963568
[rank0]:[W207 06:44:06.760549586 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W207 06:44:07.788845215 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W207 06:44:07.801932187 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W207 06:44:07.820236268 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank7]:[W207 06:44:07.823153172 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W207 06:44:07.839313168 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W207 06:44:07.875053365 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W207 06:44:07.966564003 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank7]: Traceback (most recent call last):
[rank7]:   File "/disk/weijianwang/VIS-tracking/Sperm-main/train.py", line 194, in <module>
[rank7]:     model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
[rank7]:   File "/disk/weijianwang/VIS-tracking/Sperm-main/models/usseg_model.py", line 294, in optimize_parameters
[rank7]:     self.scaler.scale(loss_path2).backward(retain_graph=False)
[rank7]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
[rank7]:     torch.autograd.backward(
[rank7]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank7]:     _engine_run_backward(
[rank7]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank7]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[rank4]: Traceback (most recent call last):
[rank4]:   File "/disk/weijianwang/VIS-tracking/Sperm-main/train.py", line 194, in <module>
[rank4]:     model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
[rank4]:   File "/disk/weijianwang/VIS-tracking/Sperm-main/models/usseg_model.py", line 294, in optimize_parameters
[rank4]:     self.scaler.scale(loss_path2).backward(retain_graph=False)
[rank4]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
[rank4]:     torch.autograd.backward(
[rank4]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[rank3]: Traceback (most recent call last):
[rank3]:   File "/disk/weijianwang/VIS-tracking/Sperm-main/train.py", line 194, in <module>
[rank3]:     model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
[rank3]:   File "/disk/weijianwang/VIS-tracking/Sperm-main/models/usseg_model.py", line 294, in optimize_parameters
[rank3]:     self.scaler.scale(loss_path2).backward(retain_graph=False)
[rank3]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
[rank3]:     torch.autograd.backward(
[rank3]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[rank0]: Traceback (most recent call last):
[rank0]:   File "/disk/weijianwang/VIS-tracking/Sperm-main/train.py", line 194, in <module>
[rank0]:     model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
[rank0]:   File "/disk/weijianwang/VIS-tracking/Sperm-main/models/usseg_model.py", line 294, in optimize_parameters
[rank0]:     self.scaler.scale(loss_path2).backward(retain_graph=False)
[rank0]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[rank6]: Traceback (most recent call last):
[rank6]:   File "/disk/weijianwang/VIS-tracking/Sperm-main/train.py", line 194, in <module>
[rank6]:     model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
[rank6]:   File "/disk/weijianwang/VIS-tracking/Sperm-main/models/usseg_model.py", line 294, in optimize_parameters
[rank6]:     self.scaler.scale(loss_path2).backward(retain_graph=False)
[rank6]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
[rank6]:     torch.autograd.backward(
[rank6]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank6]:     _engine_run_backward(
[rank6]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank6]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[rank5]: Traceback (most recent call last):
[rank5]:   File "/disk/weijianwang/VIS-tracking/Sperm-main/train.py", line 194, in <module>
[rank5]:     model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
[rank5]:   File "/disk/weijianwang/VIS-tracking/Sperm-main/models/usseg_model.py", line 294, in optimize_parameters
[rank5]:     self.scaler.scale(loss_path2).backward(retain_graph=False)
[rank5]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
[rank5]:     torch.autograd.backward(
[rank5]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank5]:     _engine_run_backward(
[rank5]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank5]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank5]: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[rank2]: Traceback (most recent call last):
[rank2]:   File "/disk/weijianwang/VIS-tracking/Sperm-main/train.py", line 194, in <module>
[rank2]:     model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
[rank2]:   File "/disk/weijianwang/VIS-tracking/Sperm-main/models/usseg_model.py", line 294, in optimize_parameters
[rank2]:     self.scaler.scale(loss_path2).backward(retain_graph=False)
[rank2]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
[rank2]:     torch.autograd.backward(
[rank2]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[rank1]: Traceback (most recent call last):
[rank1]:   File "/disk/weijianwang/VIS-tracking/Sperm-main/train.py", line 194, in <module>
[rank1]:     model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
[rank1]:   File "/disk/weijianwang/VIS-tracking/Sperm-main/models/usseg_model.py", line 294, in optimize_parameters
[rank1]:     self.scaler.scale(loss_path2).backward(retain_graph=False)
[rank1]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
[rank1]:     torch.autograd.backward(
[rank1]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
W0207 06:44:09.007815 2781032 site-packages/torch/distributed/elastic/agent/server/api.py:719] Received Signals.SIGINT death signal, shutting down workers
W0207 06:44:09.008870 2781032 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2781190 closing signal SIGINT
W0207 06:44:09.009332 2781032 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2781191 closing signal SIGINT
W0207 06:44:09.009804 2781032 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2781192 closing signal SIGINT
W0207 06:44:09.010451 2781032 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2781193 closing signal SIGINT
W0207 06:44:09.010936 2781032 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2781194 closing signal SIGINT
W0207 06:44:09.011520 2781032 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2781195 closing signal SIGINT
W0207 06:44:09.012037 2781032 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2781196 closing signal SIGINT
W0207 06:44:09.012455 2781032 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2781197 closing signal SIGINT
W0207 06:44:09.252923 2781032 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2781190 closing signal SIGTERM
W0207 06:44:09.253341 2781032 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2781191 closing signal SIGTERM
W0207 06:44:09.253461 2781032 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2781192 closing signal SIGTERM
W0207 06:44:09.253564 2781032 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2781193 closing signal SIGTERM
W0207 06:44:09.253665 2781032 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2781194 closing signal SIGTERM
W0207 06:44:09.253763 2781032 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2781195 closing signal SIGTERM
W0207 06:44:09.253859 2781032 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2781196 closing signal SIGTERM
W0207 06:44:09.253955 2781032 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2781197 closing signal SIGTERM
Traceback (most recent call last):
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 711, in run
    result = self._invoke_run(role)
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 870, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2781032 got signal: 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/weijianwang/anaconda3/envs/VIS/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 260, in launch_agent
    result = agent.run()
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 720, in run
    self._shutdown(e.sigval)
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 372, in _shutdown
    self._pcontext.close(death_sig)
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 572, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 909, in _close
    handler.proc.wait(time_to_wait)
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/subprocess.py", line 1204, in wait
    return self._wait(timeout=timeout)
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/subprocess.py", line 1932, in _wait
    time.sleep(delay)
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2781032 got signal: 2
