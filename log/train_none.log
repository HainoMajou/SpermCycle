W0106 07:12:08.682983 845702 site-packages/torch/distributed/run.py:792] 
W0106 07:12:08.682983 845702 site-packages/torch/distributed/run.py:792] *****************************************
W0106 07:12:08.682983 845702 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0106 07:12:08.682983 845702 site-packages/torch/distributed/run.py:792] *****************************************
----------------- Options ---------------
               batch_size: 1                             	[default: 2]
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: False                         
                crop_size: 800                           	[default: 512]
                 dataroot: ../datasets/sperm_instance_800	[default: None]
             dataset_mode: usseg                         	[default: template]
                direction: AtoB                          
             display_freq: 200                           
              distributed: False                         	[default: None]
                    epoch: latest                        
              epoch_count: 1                             
                 gan_mode: lsgan                         
                  gpu_ids: 2,3,5                         	[default: 0,1]
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 1                             
                  isTrain: True                          	[default: None]
                 lambda_A: 1.0                           	[default: 10.0]
                 lambda_B: 1.0                           	[default: 10.0]
                load_iter: 0                             	[default: 0]
                load_size: 800                           	[default: 512]
               local_rank: 0                             	[default: None]
                       lr: 0.0002                        
           lr_decay_iters: 100                           	[default: 50]
                lr_policy: step                          	[default: linear]
              mask_thresh: 0.5                           
         max_dataset_size: inf                           
            max_instances: 7                             	[default: 5]
                    model: usseg                         	[default: cycle_gan]
                 n_epochs: 200                           	[default: 100]
           n_epochs_decay: 300                           	[default: 100]
               n_layers_D: 3                             
                     name: none2                         	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: resnet_9blocks                
                      ngf: 64                            
               no_dropout: True                          
                  no_flip: True                          	[default: False]
                     norm: instance                      
              num_threads: 4                             	[default: 8]
                output_nc: 1                             
                    phase: train                         
                pool_size: 50                            
               preprocess: none                          	[default: resize_and_crop]
           pretrained_seg: False                         
               print_freq: 100                           
                     rank: 0                             	[default: None]
             save_by_iter: False                         
          save_epoch_freq: 100                           
         save_latest_freq: 4000                          
             score_thresh: 0.5                           
           serial_batches: False                         
                   suffix:                               
                  use_amp: True                          	[default: False]
                  verbose: False                         
               world_size: 1                             	[default: None]
----------------- End -------------------
dataset [USSEGDataset] was created
Using DistributedSampler: 200 samples per GPU
DDP enabled: world_size=3, using DistributedSampler
The number of training images = 600 (total), 200 per GPU
Some weights of SwinBackbone were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized: ['hidden_states_norms.stage1.bias', 'hidden_states_norms.stage1.weight', 'hidden_states_norms.stage2.bias', 'hidden_states_norms.stage2.weight', 'hidden_states_norms.stage3.bias', 'hidden_states_norms.stage3.weight', 'hidden_states_norms.stage4.bias', 'hidden_states_norms.stage4.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of SwinBackbone were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized: ['hidden_states_norms.stage1.bias', 'hidden_states_norms.stage1.weight', 'hidden_states_norms.stage2.bias', 'hidden_states_norms.stage2.weight', 'hidden_states_norms.stage3.bias', 'hidden_states_norms.stage3.weight', 'hidden_states_norms.stage4.bias', 'hidden_states_norms.stage4.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.weight torch.Size([96, 3, 4, 4])
model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.bias torch.Size([96])
model.pixel_level_module.encoder.embeddings.norm.weight torch.Size([96])
model.pixel_level_module.encoder.embeddings.norm.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.weight torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table torch.Size([169, 3])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.weight torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.weight torch.Size([384, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.weight torch.Size([96, 384])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.weight torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table torch.Size([169, 3])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.weight torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.weight torch.Size([384, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.weight torch.Size([96, 384])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.downsample.reduction.weight torch.Size([192, 384])
model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.weight torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table torch.Size([169, 6])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.weight torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.weight torch.Size([768, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.weight torch.Size([192, 768])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.weight torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table torch.Size([169, 6])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.weight torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.weight torch.Size([768, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.weight torch.Size([192, 768])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.downsample.reduction.weight torch.Size([384, 768])
model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.weight torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.downsample.reduction.weight torch.Size([768, 1536])
model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.weight torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.weight torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table torch.Size([169, 24])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.weight torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.weight torch.Size([3072, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.bias torch.Size([3072])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.weight torch.Size([768, 3072])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.weight torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table torch.Size([169, 24])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.weight torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.weight torch.Size([3072, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.bias torch.Size([3072])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.weight torch.Size([768, 3072])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.bias torch.Size([768])
model.pixel_level_module.encoder.hidden_states_norms.stage1.weight torch.Size([96])
model.pixel_level_module.encoder.hidden_states_norms.stage1.bias torch.Size([96])
model.pixel_level_module.encoder.hidden_states_norms.stage2.weight torch.Size([192])
model.pixel_level_module.encoder.hidden_states_norms.stage2.bias torch.Size([192])
model.pixel_level_module.encoder.hidden_states_norms.stage3.weight torch.Size([384])
model.pixel_level_module.encoder.hidden_states_norms.stage3.bias torch.Size([384])
model.pixel_level_module.encoder.hidden_states_norms.stage4.weight torch.Size([768])
model.pixel_level_module.encoder.hidden_states_norms.stage4.bias torch.Size([768])
model.pixel_level_module.decoder.level_embed torch.Size([3, 256])
model.pixel_level_module.decoder.input_projections.0.0.weight torch.Size([256, 768, 1, 1])
model.pixel_level_module.decoder.input_projections.0.0.bias torch.Size([256])
model.pixel_level_module.decoder.input_projections.0.1.weight torch.Size([256])
model.pixel_level_module.decoder.input_projections.0.1.bias torch.Size([256])
model.pixel_level_module.decoder.input_projections.1.0.weight torch.Size([256, 384, 1, 1])
model.pixel_level_module.decoder.input_projections.1.0.bias torch.Size([256])
model.pixel_level_module.decoder.input_projections.1.1.weight torch.Size([256])
model.pixel_level_module.decoder.input_projections.1.1.bias torch.Size([256])
model.pixel_level_module.decoder.input_projections.2.0.weight torch.Size([256, 192, 1, 1])
model.pixel_level_module.decoder.input_projections.2.0.bias torch.Size([256])
model.pixel_level_module.decoder.input_projections.2.1.weight torch.Size([256])
model.pixel_level_module.decoder.input_projections.2.1.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.0.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.0.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.0.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.1.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.1.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.1.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.2.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.2.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.2.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.3.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.3.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.3.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.4.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.4.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.4.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.5.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.5.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.5.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.mask_projection.weight torch.Size([256, 256, 1, 1])
model.pixel_level_module.decoder.mask_projection.bias torch.Size([256])
model.pixel_level_module.decoder.adapter_1.0.weight torch.Size([256, 96, 1, 1])
model.pixel_level_module.decoder.adapter_1.1.weight torch.Size([256])
model.pixel_level_module.decoder.adapter_1.1.bias torch.Size([256])
model.pixel_level_module.decoder.layer_1.0.weight torch.Size([256, 256, 3, 3])
model.pixel_level_module.decoder.layer_1.1.weight torch.Size([256])
model.pixel_level_module.decoder.layer_1.1.bias torch.Size([256])
model.transformer_module.queries_embedder.weight torch.Size([7, 256])
model.transformer_module.queries_features.weight torch.Size([7, 256])
model.transformer_module.decoder.layers.0.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.0.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.0.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.0.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.0.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.0.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.0.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.0.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.0.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.0.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.0.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.0.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.0.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.0.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.0.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.0.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.0.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.0.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.0.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.0.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.0.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.0.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.1.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.1.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.1.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.1.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.1.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.1.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.1.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.1.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.1.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.1.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.1.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.1.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.1.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.1.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.1.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.1.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.2.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.2.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.2.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.2.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.2.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.2.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.2.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.2.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.2.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.2.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.2.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.2.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.2.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.2.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.2.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.2.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.3.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.3.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.3.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.3.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.3.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.3.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.3.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.3.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.3.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.3.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.3.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.3.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.3.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.3.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.3.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.3.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.4.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.4.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.4.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.4.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.4.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.4.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.4.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.4.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.4.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.4.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.4.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.4.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.4.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.4.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.4.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.4.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.5.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.5.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.5.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.5.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.5.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.5.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.5.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.5.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.5.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.5.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.5.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.5.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.5.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.5.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.5.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.5.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.6.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.6.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.6.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.6.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.6.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.6.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.6.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.6.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.6.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.6.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.6.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.6.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.6.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.6.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.6.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.6.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.7.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.7.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.7.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.7.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.7.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.7.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.7.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.7.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.7.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.7.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.7.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.7.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.7.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.7.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.7.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.7.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.8.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.8.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.8.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.8.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.8.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.8.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.8.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.8.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.8.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.8.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.8.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.8.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.8.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.8.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.8.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.8.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layernorm.weight torch.Size([256])
model.transformer_module.decoder.layernorm.bias torch.Size([256])
model.transformer_module.decoder.mask_predictor.mask_embedder.0.0.weight torch.Size([256, 256])
model.transformer_module.decoder.mask_predictor.mask_embedder.0.0.bias torch.Size([256])
model.transformer_module.decoder.mask_predictor.mask_embedder.1.0.weight torch.Size([256, 256])
model.transformer_module.decoder.mask_predictor.mask_embedder.1.0.bias torch.Size([256])
model.transformer_module.decoder.mask_predictor.mask_embedder.2.0.weight torch.Size([256, 256])
model.transformer_module.decoder.mask_predictor.mask_embedder.2.0.bias torch.Size([256])
model.transformer_module.level_embed.weight torch.Size([3, 256])
class_predictor.weight torch.Size([2, 256])
class_predictor.bias torch.Size([2])
pixel_encoder_params:  27520698
pixel_decoder_params:  5421504
transformer_params:  14411008
model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.weight torch.Size([96, 3, 4, 4])
model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.bias torch.Size([96])
model.pixel_level_module.encoder.embeddings.norm.weight torch.Size([96])
model.pixel_level_module.encoder.embeddings.norm.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.weight torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table torch.Size([169, 3])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.weight torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.weight torch.Size([384, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.weight torch.Size([96, 384])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.weight torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table torch.Size([169, 3])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.weight torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.weight torch.Size([384, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.weight torch.Size([96, 384])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.downsample.reduction.weight torch.Size([192, 384])
model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.weight torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table torch.Size([169, 6])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.weight torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.weight torch.Size([768, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.weight torch.Size([192, 768])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.weight torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table torch.Size([169, 6])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.weight torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.weight torch.Size([768, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.weight torch.Size([192, 768])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.downsample.reduction.weight torch.Size([384, 768])
model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.weight torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.downsample.reduction.weight torch.Size([768, 1536])
model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.weight torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.weight torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table torch.Size([169, 24])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.weight torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.weight torch.Size([3072, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.bias torch.Size([3072])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.weight torch.Size([768, 3072])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.weight torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table torch.Size([169, 24])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.weight torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.weight torch.Size([3072, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.bias torch.Size([3072])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.weight torch.Size([768, 3072])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.bias torch.Size([768])
model.pixel_level_module.encoder.hidden_states_norms.stage1.weight torch.Size([96])
model.pixel_level_module.encoder.hidden_states_norms.stage1.bias torch.Size([96])
model.pixel_level_module.encoder.hidden_states_norms.stage2.weight torch.Size([192])
model.pixel_level_module.encoder.hidden_states_norms.stage2.bias torch.Size([192])
model.pixel_level_module.encoder.hidden_states_norms.stage3.weight torch.Size([384])
model.pixel_level_module.encoder.hidden_states_norms.stage3.bias torch.Size([384])
model.pixel_level_module.encoder.hidden_states_norms.stage4.weight torch.Size([768])
model.pixel_level_module.encoder.hidden_states_norms.stage4.bias torch.Size([768])
model.pixel_level_module.decoder.level_embed torch.Size([3, 256])
model.pixel_level_module.decoder.input_projections.0.0.weight torch.Size([256, 768, 1, 1])
model.pixel_level_module.decoder.input_projections.0.0.bias torch.Size([256])
model.pixel_level_module.decoder.input_projections.0.1.weight torch.Size([256])
model.pixel_level_module.decoder.input_projections.0.1.bias torch.Size([256])
model.pixel_level_module.decoder.input_projections.1.0.weight torch.Size([256, 384, 1, 1])
model.pixel_level_module.decoder.input_projections.1.0.bias torch.Size([256])
model.pixel_level_module.decoder.input_projections.1.1.weight torch.Size([256])
model.pixel_level_module.decoder.input_projections.1.1.bias torch.Size([256])
model.pixel_level_module.decoder.input_projections.2.0.weight torch.Size([256, 192, 1, 1])
model.pixel_level_module.decoder.input_projections.2.0.bias torch.Size([256])
model.pixel_level_module.decoder.input_projections.2.1.weight torch.Size([256])
model.pixel_level_module.decoder.input_projections.2.1.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.0.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.0.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.0.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.1.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.1.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.1.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.2.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.2.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.2.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.3.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.3.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.3.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.4.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.4.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.4.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.5.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.5.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.5.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.mask_projection.weight torch.Size([256, 256, 1, 1])
model.pixel_level_module.decoder.mask_projection.bias torch.Size([256])
model.pixel_level_module.decoder.adapter_1.0.weight torch.Size([256, 96, 1, 1])
model.pixel_level_module.decoder.adapter_1.1.weight torch.Size([256])
model.pixel_level_module.decoder.adapter_1.1.bias torch.Size([256])
model.pixel_level_module.decoder.layer_1.0.weight torch.Size([256, 256, 3, 3])
model.pixel_level_module.decoder.layer_1.1.weight torch.Size([256])
model.pixel_level_module.decoder.layer_1.1.bias torch.Size([256])
model.transformer_module.queries_embedder.weight torch.Size([7, 256])
model.transformer_module.queries_features.weight torch.Size([7, 256])
model.transformer_module.decoder.layers.0.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.0.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.0.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.0.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.0.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.0.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.0.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.0.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.0.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.0.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.0.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.0.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.0.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.0.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.0.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.0.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.0.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.0.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.0.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.0.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.0.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.0.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.1.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.1.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.1.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.1.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.1.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.1.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.1.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.1.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.1.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.1.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.1.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.1.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.1.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.1.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.1.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.1.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.2.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.2.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.2.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.2.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.2.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.2.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.2.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.2.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.2.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.2.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.2.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.2.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.2.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.2.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.2.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.2.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.3.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.3.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.3.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.3.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.3.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.3.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.3.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.3.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.3.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.3.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.3.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.3.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.3.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.3.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.3.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.3.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.4.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.4.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.4.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.4.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.4.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.4.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.4.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.4.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.4.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.4.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.4.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.4.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.4.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.4.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.4.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.4.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.5.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.5.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.5.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.5.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.5.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.5.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.5.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.5.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.5.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.5.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.5.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.5.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.5.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.5.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.5.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.5.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.6.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.6.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.6.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.6.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.6.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.6.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.6.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.6.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.6.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.6.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.6.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.6.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.6.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.6.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.6.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.6.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.7.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.7.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.7.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.7.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.7.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.7.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.7.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.7.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.7.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.7.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.7.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.7.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.7.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.7.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.7.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.7.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.8.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.8.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.8.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.8.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.8.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.8.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.8.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.8.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.8.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.8.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.8.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.8.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.8.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.8.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.8.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.8.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layernorm.weight torch.Size([256])
model.transformer_module.decoder.layernorm.bias torch.Size([256])
model.transformer_module.decoder.mask_predictor.mask_embedder.0.0.weight torch.Size([256, 256])
model.transformer_module.decoder.mask_predictor.mask_embedder.0.0.bias torch.Size([256])
model.transformer_module.decoder.mask_predictor.mask_embedder.1.0.weight torch.Size([256, 256])
model.transformer_module.decoder.mask_predictor.mask_embedder.1.0.bias torch.Size([256])
model.transformer_module.decoder.mask_predictor.mask_embedder.2.0.weight torch.Size([256, 256])
model.transformer_module.decoder.mask_predictor.mask_embedder.2.0.bias torch.Size([256])
model.transformer_module.level_embed.weight torch.Size([3, 256])
class_predictor.weight torch.Size([2, 256])
class_predictor.bias torch.Size([2])
pixel_encoder_params:  27520698
pixel_decoder_params:  5421504
transformer_params:  14411008
Some weights of SwinBackbone were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized: ['hidden_states_norms.stage1.bias', 'hidden_states_norms.stage1.weight', 'hidden_states_norms.stage2.bias', 'hidden_states_norms.stage2.weight', 'hidden_states_norms.stage3.bias', 'hidden_states_norms.stage3.weight', 'hidden_states_norms.stage4.bias', 'hidden_states_norms.stage4.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.weight torch.Size([96, 3, 4, 4])
model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.bias torch.Size([96])
model.pixel_level_module.encoder.embeddings.norm.weight torch.Size([96])
model.pixel_level_module.encoder.embeddings.norm.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.weight torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table torch.Size([169, 3])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.weight torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.weight torch.Size([384, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.weight torch.Size([96, 384])
model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.weight torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table torch.Size([169, 3])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.weight torch.Size([96, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.weight torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.weight torch.Size([384, 96])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.weight torch.Size([96, 384])
model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.bias torch.Size([96])
model.pixel_level_module.encoder.encoder.layers.0.downsample.reduction.weight torch.Size([192, 384])
model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.weight torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table torch.Size([169, 6])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.weight torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.weight torch.Size([768, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.weight torch.Size([192, 768])
model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.weight torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table torch.Size([169, 6])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.weight torch.Size([192, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.weight torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.weight torch.Size([768, 192])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.weight torch.Size([192, 768])
model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.bias torch.Size([192])
model.pixel_level_module.encoder.encoder.layers.1.downsample.reduction.weight torch.Size([384, 768])
model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.weight torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table torch.Size([169, 12])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.weight torch.Size([384, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.weight torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.weight torch.Size([1536, 384])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.weight torch.Size([384, 1536])
model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.bias torch.Size([384])
model.pixel_level_module.encoder.encoder.layers.2.downsample.reduction.weight torch.Size([768, 1536])
model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.weight torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.bias torch.Size([1536])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.weight torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table torch.Size([169, 24])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.weight torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.weight torch.Size([3072, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.bias torch.Size([3072])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.weight torch.Size([768, 3072])
model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.weight torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table torch.Size([169, 24])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.weight torch.Size([768, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.weight torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.bias torch.Size([768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.weight torch.Size([3072, 768])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.bias torch.Size([3072])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.weight torch.Size([768, 3072])
model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.bias torch.Size([768])
model.pixel_level_module.encoder.hidden_states_norms.stage1.weight torch.Size([96])
model.pixel_level_module.encoder.hidden_states_norms.stage1.bias torch.Size([96])
model.pixel_level_module.encoder.hidden_states_norms.stage2.weight torch.Size([192])
model.pixel_level_module.encoder.hidden_states_norms.stage2.bias torch.Size([192])
model.pixel_level_module.encoder.hidden_states_norms.stage3.weight torch.Size([384])
model.pixel_level_module.encoder.hidden_states_norms.stage3.bias torch.Size([384])
model.pixel_level_module.encoder.hidden_states_norms.stage4.weight torch.Size([768])
model.pixel_level_module.encoder.hidden_states_norms.stage4.bias torch.Size([768])
model.pixel_level_module.decoder.level_embed torch.Size([3, 256])
model.pixel_level_module.decoder.input_projections.0.0.weight torch.Size([256, 768, 1, 1])
model.pixel_level_module.decoder.input_projections.0.0.bias torch.Size([256])
model.pixel_level_module.decoder.input_projections.0.1.weight torch.Size([256])
model.pixel_level_module.decoder.input_projections.0.1.bias torch.Size([256])
model.pixel_level_module.decoder.input_projections.1.0.weight torch.Size([256, 384, 1, 1])
model.pixel_level_module.decoder.input_projections.1.0.bias torch.Size([256])
model.pixel_level_module.decoder.input_projections.1.1.weight torch.Size([256])
model.pixel_level_module.decoder.input_projections.1.1.bias torch.Size([256])
model.pixel_level_module.decoder.input_projections.2.0.weight torch.Size([256, 192, 1, 1])
model.pixel_level_module.decoder.input_projections.2.0.bias torch.Size([256])
model.pixel_level_module.decoder.input_projections.2.1.weight torch.Size([256])
model.pixel_level_module.decoder.input_projections.2.1.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.0.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.0.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.0.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.0.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.1.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.1.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.1.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.1.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.2.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.2.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.2.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.3.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.3.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.3.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.4.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.4.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.4.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.weight torch.Size([192, 256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.bias torch.Size([192])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.weight torch.Size([96, 256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.bias torch.Size([96])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.weight torch.Size([256, 256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.fc1.weight torch.Size([1024, 256])
model.pixel_level_module.decoder.encoder.layers.5.fc1.bias torch.Size([1024])
model.pixel_level_module.decoder.encoder.layers.5.fc2.weight torch.Size([256, 1024])
model.pixel_level_module.decoder.encoder.layers.5.fc2.bias torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.weight torch.Size([256])
model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.bias torch.Size([256])
model.pixel_level_module.decoder.mask_projection.weight torch.Size([256, 256, 1, 1])
model.pixel_level_module.decoder.mask_projection.bias torch.Size([256])
model.pixel_level_module.decoder.adapter_1.0.weight torch.Size([256, 96, 1, 1])
model.pixel_level_module.decoder.adapter_1.1.weight torch.Size([256])
model.pixel_level_module.decoder.adapter_1.1.bias torch.Size([256])
model.pixel_level_module.decoder.layer_1.0.weight torch.Size([256, 256, 3, 3])
model.pixel_level_module.decoder.layer_1.1.weight torch.Size([256])
model.pixel_level_module.decoder.layer_1.1.bias torch.Size([256])
model.transformer_module.queries_embedder.weight torch.Size([7, 256])
model.transformer_module.queries_features.weight torch.Size([7, 256])
model.transformer_module.decoder.layers.0.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.0.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.0.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.0.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.0.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.0.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.0.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.0.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.0.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.0.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.0.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.0.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.0.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.0.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.0.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.0.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.0.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.0.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.0.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.0.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.0.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.0.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.1.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.1.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.1.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.1.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.1.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.1.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.1.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.1.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.1.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.1.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.1.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.1.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.1.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.1.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.1.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.1.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.1.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.2.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.2.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.2.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.2.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.2.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.2.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.2.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.2.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.2.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.2.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.2.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.2.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.2.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.2.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.2.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.2.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.2.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.3.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.3.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.3.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.3.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.3.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.3.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.3.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.3.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.3.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.3.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.3.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.3.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.3.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.3.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.3.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.3.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.3.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.4.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.4.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.4.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.4.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.4.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.4.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.4.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.4.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.4.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.4.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.4.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.4.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.4.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.4.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.4.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.4.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.4.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.5.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.5.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.5.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.5.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.5.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.5.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.5.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.5.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.5.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.5.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.5.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.5.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.5.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.5.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.5.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.5.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.5.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.6.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.6.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.6.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.6.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.6.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.6.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.6.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.6.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.6.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.6.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.6.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.6.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.6.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.6.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.6.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.6.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.6.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.7.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.7.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.7.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.7.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.7.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.7.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.7.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.7.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.7.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.7.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.7.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.7.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.7.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.7.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.7.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.7.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.7.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn.k_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.8.self_attn.k_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn.v_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.8.self_attn.v_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn.q_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.8.self_attn.q_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.8.self_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.8.self_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.8.cross_attn.in_proj_weight torch.Size([768, 256])
model.transformer_module.decoder.layers.8.cross_attn.in_proj_bias torch.Size([768])
model.transformer_module.decoder.layers.8.cross_attn.out_proj.weight torch.Size([256, 256])
model.transformer_module.decoder.layers.8.cross_attn.out_proj.bias torch.Size([256])
model.transformer_module.decoder.layers.8.cross_attn_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.8.cross_attn_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layers.8.fc1.weight torch.Size([2048, 256])
model.transformer_module.decoder.layers.8.fc1.bias torch.Size([2048])
model.transformer_module.decoder.layers.8.fc2.weight torch.Size([256, 2048])
model.transformer_module.decoder.layers.8.fc2.bias torch.Size([256])
model.transformer_module.decoder.layers.8.final_layer_norm.weight torch.Size([256])
model.transformer_module.decoder.layers.8.final_layer_norm.bias torch.Size([256])
model.transformer_module.decoder.layernorm.weight torch.Size([256])
model.transformer_module.decoder.layernorm.bias torch.Size([256])
model.transformer_module.decoder.mask_predictor.mask_embedder.0.0.weight torch.Size([256, 256])
model.transformer_module.decoder.mask_predictor.mask_embedder.0.0.bias torch.Size([256])
model.transformer_module.decoder.mask_predictor.mask_embedder.1.0.weight torch.Size([256, 256])
model.transformer_module.decoder.mask_predictor.mask_embedder.1.0.bias torch.Size([256])
model.transformer_module.decoder.mask_predictor.mask_embedder.2.0.weight torch.Size([256, 256])
model.transformer_module.decoder.mask_predictor.mask_embedder.2.0.bias torch.Size([256])
model.transformer_module.level_embed.weight torch.Size([3, 256])
class_predictor.weight torch.Size([2, 256])
class_predictor.bias torch.Size([2])
pixel_encoder_params:  27520698
pixel_decoder_params:  5421504
transformer_params:  14411008
initialize network with normal
initialize network with normal
model [USSEGModel] was created
---------- Networks initialized -------------
[Network G_A] Total number of parameters : 47.354 M
[Network G_B] Total number of parameters : 12.546 M
[Network D_B] Total number of parameters : 2.763 M
-----------------------------------------------
create image directory ./checkpoints/none2/images...
[rank2]:[W106 07:12:24.284952372 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W106 07:12:25.352699107 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W106 07:12:25.419332934 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/weijianwang/VIS-tracking/Sperm-main/train.py", line 174, in <module>
[rank0]:     model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
[rank0]:   File "/home/weijianwang/VIS-tracking/Sperm-main/models/usseg_model.py", line 245, in optimize_parameters
[rank0]:     self.loss_cycle_A = self.criterionInstanceCycle(self.rec_A_logits, self.real_A, self.rec_A_classes) * self.opt.lambda_A
[rank0]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/weijianwang/VIS-tracking/Sperm-main/models/networks.py", line 929, in forward
[rank0]:     pred_idx, tgt_idx = self._hungarian_match(p_masks, valid_t, p_probs)
[rank0]:   File "/home/weijianwang/VIS-tracking/Sperm-main/models/networks.py", line 885, in _hungarian_match
[rank0]:     cost_mask = F.binary_cross_entropy(out_prob.repeat(1, M, 1), tgt_prob.repeat(N, 1, 1), reduction='none').mean(-1)
[rank0]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/nn/functional.py", line 3569, in binary_cross_entropy
[rank0]:     return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
[rank0]: RuntimeError: torch.nn.functional.binary_cross_entropy and torch.nn.BCELoss are unsafe to autocast.
[rank0]: Many models use a sigmoid layer right before the binary cross entropy layer.
[rank0]: In this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits
[rank0]: or torch.nn.BCEWithLogitsLoss.  binary_cross_entropy_with_logits and BCEWithLogits are
[rank0]: safe to autocast.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/weijianwang/VIS-tracking/Sperm-main/train.py", line 174, in <module>
[rank2]:     model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
[rank2]:   File "/home/weijianwang/VIS-tracking/Sperm-main/models/usseg_model.py", line 245, in optimize_parameters
[rank2]:     self.loss_cycle_A = self.criterionInstanceCycle(self.rec_A_logits, self.real_A, self.rec_A_classes) * self.opt.lambda_A
[rank2]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/weijianwang/VIS-tracking/Sperm-main/models/networks.py", line 929, in forward
[rank2]:     pred_idx, tgt_idx = self._hungarian_match(p_masks, valid_t, p_probs)
[rank2]:   File "/home/weijianwang/VIS-tracking/Sperm-main/models/networks.py", line 885, in _hungarian_match
[rank2]:     cost_mask = F.binary_cross_entropy(out_prob.repeat(1, M, 1), tgt_prob.repeat(N, 1, 1), reduction='none').mean(-1)
[rank2]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/nn/functional.py", line 3569, in binary_cross_entropy
[rank2]:     return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
[rank2]: RuntimeError: torch.nn.functional.binary_cross_entropy and torch.nn.BCELoss are unsafe to autocast.
[rank2]: Many models use a sigmoid layer right before the binary cross entropy layer.
[rank2]: In this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits
[rank2]: or torch.nn.BCEWithLogitsLoss.  binary_cross_entropy_with_logits and BCEWithLogits are
[rank2]: safe to autocast.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/weijianwang/VIS-tracking/Sperm-main/train.py", line 174, in <module>
[rank1]:     model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
[rank1]:   File "/home/weijianwang/VIS-tracking/Sperm-main/models/usseg_model.py", line 245, in optimize_parameters
[rank1]:     self.loss_cycle_A = self.criterionInstanceCycle(self.rec_A_logits, self.real_A, self.rec_A_classes) * self.opt.lambda_A
[rank1]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/weijianwang/VIS-tracking/Sperm-main/models/networks.py", line 929, in forward
[rank1]:     pred_idx, tgt_idx = self._hungarian_match(p_masks, valid_t, p_probs)
[rank1]:   File "/home/weijianwang/VIS-tracking/Sperm-main/models/networks.py", line 885, in _hungarian_match
[rank1]:     cost_mask = F.binary_cross_entropy(out_prob.repeat(1, M, 1), tgt_prob.repeat(N, 1, 1), reduction='none').mean(-1)
[rank1]:   File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/nn/functional.py", line 3569, in binary_cross_entropy
[rank1]:     return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
[rank1]: RuntimeError: torch.nn.functional.binary_cross_entropy and torch.nn.BCELoss are unsafe to autocast.
[rank1]: Many models use a sigmoid layer right before the binary cross entropy layer.
[rank1]: In this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits
[rank1]: or torch.nn.BCEWithLogitsLoss.  binary_cross_entropy_with_logits and BCEWithLogits are
[rank1]: safe to autocast.
W0106 07:12:30.473814 845702 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 845913 closing signal SIGTERM
W0106 07:12:30.474726 845702 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 845915 closing signal SIGTERM
E0106 07:12:30.940192 845702 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 845914) of binary: /home/weijianwang/anaconda3/envs/VIS/bin/python3.10
Traceback (most recent call last):
  File "/home/weijianwang/anaconda3/envs/VIS/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/weijianwang/anaconda3/envs/VIS/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-01-06_07:12:30
  host      : asus
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 845914)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
